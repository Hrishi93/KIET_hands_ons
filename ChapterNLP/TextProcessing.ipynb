{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef0f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization  \n",
    "#With out using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d132f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'i', 'am', 'Hrishikesh', 'Mahure,', 'taking', 'AICW', 'session', 'at', 'KIET,', 'from', 'Edunet,', 'It', 'is', 'a', 'Microsoft', 'Program']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hello i am Hrishikesh Mahure, \n",
    "         taking AICW session at KIET, from Edunet, \n",
    "         It is a Microsoft Program\"\"\"\n",
    "\n",
    "word_token = text.split()\n",
    "print(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ee28b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello i am Hrishikesh Mahure', ' \\n         taking AICW session at KIET', ' from Edunet', ' \\n         It is a Microsoft Program']\n"
     ]
    }
   ],
   "source": [
    "sent_token = text.split(',')\n",
    "print(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c9a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sonum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenazation using nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36227a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'i', 'am', 'Hrishikesh', 'Mahure', ',', 'taking', 'AICW', 'session', 'at', 'KIET', ',', 'from', 'Edunet', ',', 'It', 'is', 'a', 'Microsoft', 'Program']\n"
     ]
    }
   ],
   "source": [
    "#word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"Hello i am Hrishikesh Mahure, \n",
    "         taking AICW session at KIET, from Edunet, \n",
    "         It is a Microsoft Program\"\"\"\n",
    "\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26032355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello i am Hrishikesh Mahure, \\n         taking AICW session at KIET, from Edunet, \\n         It is a Microsoft Program']\n"
     ]
    }
   ],
   "source": [
    "#Sentance Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"\"\"Hello i am Hrishikesh Mahure, \n",
    "         taking AICW session at KIET, from Edunet, \n",
    "         It is a Microsoft Program\"\"\"\n",
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aba8a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sonum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopword Removal\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) #assign stop words\n",
    "\n",
    "text = \"\"\"my name is Hrk from Nagpur we are going \n",
    "to conduct session in KIET it is very important for\n",
    "AIML students\"\"\"\n",
    "\n",
    "#step 1 convert text in token\n",
    "word_token = nltk.word_tokenize(text)\n",
    "\n",
    "#Step 2 - Remove the Stowords\n",
    "after_remove_text = []  #empty String\n",
    "\n",
    "for w in word_token:\n",
    "    if w not in stop_words:\n",
    "        after_remove_text.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94bf6266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is Hrk from Nagpur we are going \n",
      "to conduct session in KIET it is very important for\n",
      "AIML students\n",
      "------------------\n",
      "['name', 'Hrk', 'Nagpur', 'going', 'conduct', 'session', 'KIET', 'important', 'AIML', 'students']\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(\"------------------\")\n",
    "print(after_remove_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15334098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hrishikesh  :  NNP\n",
      "Running  :  NNP\n",
      "on  :  IN\n",
      "a  :  DT\n",
      "track  :  NN\n",
      "of  :  IN\n",
      "KIET  :  NNP\n",
      "college  :  NN\n",
      ",  :  ,\n",
      "that  :  WDT\n",
      "was  :  VBD\n",
      "a  :  DT\n",
      "very  :  RB\n",
      "good  :  JJ\n",
      "experience  :  NN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sonum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sonum\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#3 Part of Speech = Tagging\n",
    "#Hrishikesh  - Noun\n",
    "#Running - VErb\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('punkt') #Tokens\n",
    "nltk.download('averaged_perceptron_tagger_eng') #POS\n",
    "\n",
    "text = \"\"\" Hrishikesh Running on a track of KIET college,\n",
    "            that was a very good experience\"\"\"\n",
    "\n",
    "#Toeknization \n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "#POS tagging\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "\n",
    "#Display Tags\n",
    "for w , pos_tag in pos_tags:\n",
    "    print(w , \" : \" , pos_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eac26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Word\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1501c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>last</th>\n",
       "      <th>second</th>\n",
       "      <th>so</th>\n",
       "      <th>then</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>who</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This  is  a   Document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This  is  a   second  Document</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And   This  is   Third</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>So  it  is  First</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then  who  is  last</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                and  document  first  is  it  last  second  \\\n",
       "This  is  a   Document            0         1      0   1   0     0       0   \n",
       "This  is  a   second  Document    0         1      0   1   0     0       1   \n",
       "And   This  is   Third            1         0      0   1   0     0       0   \n",
       "So  it  is  First                 0         0      1   1   1     0       0   \n",
       "then  who  is  last               0         0      0   1   0     1       0   \n",
       "\n",
       "                                so  then  third  this  who  \n",
       "This  is  a   Document           0     0      0     1    0  \n",
       "This  is  a   second  Document   0     0      0     1    0  \n",
       "And   This  is   Third           0     0      1     1    0  \n",
       "So  it  is  First                1     0      0     0    0  \n",
       "then  who  is  last              0     1      0     0    1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "text = ['This  is  a   Document',\n",
    "        'This  is  a   second  Document',\n",
    "        'And   This  is   Third',\n",
    "        'So  it  is  First',\n",
    "        'then  who  is  last']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text)\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "df = pd.DataFrame(X.toarray() , columns = columns , index = text)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
